import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Load and preprocess MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()


# Normalize pixel values between 0 and 1 : xhen normalizing pixel values, the typical range for pixel values in images is between 0 and 255 
# for grayscale images (0 being black and 255 being white). 
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Reshape images to a flattened format :  each image is initially represented as a 28x28 array of pixel values (2D array). 
# To feed this data into a neural network, it needs to be converted into a single vector with 784 (28x28) elements (1D array).
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)


# Visualisation of the first six images in x_train
plt.figure(figsize=(8, 2))
for i in range(6):
    plt.subplot(1, 6, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap="gray_r") # resizes the images to 28x28 pixels
    plt.title('Label = %d' % y_train[i], fontsize=14) # displays the images along with their corresponding labels from y_train
    plt.axis("off")
plt.tight_layout()
plt.show()


# Visualisation of the shapes of the training and test data
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')


latent_dim = 2 # dimension of the latent space, it will be changes for the last question of task 3

batch_size = 128 # batch size for training


tfd = tf.contrib.distributions

def make_encoder(data, latent_dim):
    """
    Define the encoder 
    The encoder is a neural network that takes the input data and outputs a probability distribution, the approximate posterior qφ(z|x) over the latent space. 
    It learns the parameters (mean and variance) of the underlying latent distribution.
    """
    x = tf.keras.layers.Flatten()(data) #It reshapes the input data into a 1D array -> not necessary here !
    x = tf.keras.layers.Dense(256, activation='relu')(x)
    x = tf.keras.layers.Dense(256, activation='relu')(x)
    loc = tf.keras.layers.Dense(latent_dim)(x)           # outputs the mean parameters for the Multivariate Normal Diagonal distribution. It has latent_dim units.
    scale = tf.keras.layers.Dense(latent_dim, activation='softplus')(x) # outputs the scale (standard deviation) parameters for the Multivariate Normal Diagonal distribution. It also has latent_dim units and uses a softplus activation function.
    return tfd.MultivariateNormalDiag(loc, scale) # multivariate diagonal Gaussian distribution as approximate posterior qφ(z|x)



def make_prior(latent_dim):
    """
    Define the prior : a multivariate diagonal standard normal distribution
    """
    loc = tf.zeros(latent_dim)
    scale = tf.ones(latent_dim)
    return tfd.MultivariateNormalDiag(loc, scale)


def make_decoder(code, data_shape=784):
    """
    Define the decoder
    The decoder is a neural network that takes a sample from the latent space and outputs a probability distribution, the likelihood pθ(x|z) over the data space.
    """
    std_dev = tf.Variable(initial_value=1.0, trainable=True, dtype=tf.float32) # this variable serves as the standard deviation for the Gaussian distribution used in sampling : it's set to be trainable during model optimization.

    x = tf.keras.layers.Dense(256, activation='relu')(code)
    x = tf.keras.layers.Dense(256, activation='relu')(x)
    output_mean = tf.keras.layers.Dense(data_shape, activation=None)(x) #no activation function for the likelihood ->might have to change that!

    # Reparameterization trick for sampling
    def sampling(args):
        mean = args
        batch = tf.shape(mean)[0]
        dim = tf.shape(mean)[1]
        epsilon = tf.random.normal(shape=(batch, dim))
        return mean + std_dev * epsilon

    decoded = sampling(output_mean)

    decoder = tf.keras.Model(code, decoded)
    return decoder



""" Plotting the latent vector representation of a few batches of data : encode the test set and mark the different classes """

""" Visualisation of some of the reconstructed digits from the latent space (15) and the corresponding original ones """

""" Visualisation of some of the generated digits (15) """

""" Plotting the loss curve (test set), i.e., epoch vs. −LELBO"""