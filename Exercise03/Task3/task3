import tensorflow as tf
import tensorflow_probability as tfp
import numpy as np 
import matplotlib.pyplot as plt
import pandas as pd

# Load and preprocess MNIST data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()


# Normalize pixel values between 0 and 1 : xhen normalizing pixel values, the typical range for pixel values in images is between 0 and 255 
# for grayscale images (0 being black and 255 being white). 
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

# Reshape images to a flattened format :  each image is initially represented as a 28x28 array of pixel values (2D array). 
# To feed this data into a neural network, it needs to be converted into a single vector with 784 (28x28) elements (1D array).
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

"""
# Visualisation of the first six images in x_train
plt.figure(figsize=(8, 2))
for i in range(6):
    plt.subplot(1, 6, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap="gray_r") # resizes the images to 28x28 pixels
    plt.title('Label = %d' % y_train[i], fontsize=14) # displays the images along with their corresponding labels from y_train
    plt.axis("off")
plt.tight_layout()
plt.show()


# Visualisation of the shapes of the training and test data
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)
print('y_train shape:', y_train.shape)
print('y_test shape:', y_test.shape)

print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

"""


data_shape = 784 # shape of the data (28x28=784)

latent_dim = 2 # dimension of the latent space : to be changed for the last question of task 3

batch_size = 128 # batch size for training

#create encoder
inputs = tf.keras.Input(shape=data_shape)
x = tf.keras.layers.Dense(256, activation='relu')(inputs)
x = tf.keras.layers.Dense(256, activation='relu')(x)

# Mean and standard deviation outputs for the approximate posterior qφ(z|x)
z_mean = tf.keras.layers.Dense(latent_dim)(x)
z_log_var = tf.keras.layers.Dense(latent_dim)(x) # or tf.keras.layers.Activation('softplus')(z_log_var) -> positivity insurance


def sampling(args):
    """
    Sampling from the latent multivariate diagonal standard normal distribution as prior p(z).
    Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.
    """
    z_mean, z_log_var = args
    epsilon = tf.keras.backend.random_normal(shape=(tf.shape(z_mean)[0], latent_dim)) #shape=(batch_size, latent_dim)=z_mean.shape
    return z_mean + tf.exp(0.5*z_log_var) * epsilon

z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_var]) # or layers.Lambda(sampling)([z_mean, z_log_var]) , output_shape=(latent_dim,)

encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name='encoder')

#create decoder
latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')
x = tf.keras.layers.Dense(256, activation='relu')(latent_inputs)
x = tf.keras.layers.Dense(256, activation='relu')(x)

# Output only the mean of likelihood pθ(x|z)
outputs = tf.keras.layers.Dense(data_shape, activation='sigmoid')(x) #
# Implement standard deviation as a trainable variable of the model
outputs_stddev = tf.Variable(tf.ones(data_shape), dtype=tf.float32, name='decoder_stddev') #or decoder_stddev = tf.Variable(initial_value=1.0, trainable=True, dtype=tf.float32, name='decoder_stddev') to have a one floating point ??

decoder = tf.keras.Model(latent_inputs, outputs, name='decoder')

#create VAE
outputs = decoder(encoder(inputs)[2])
vae = tf.keras.Model(inputs, outputs, name='vae')


#define the loss function
reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, outputs)
reconstruction_loss *= data_shape
kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
kl_loss = tf.reduce_sum(kl_loss, axis=-1)
kl_loss *= -0.5

vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)
vae.add_loss(vae_loss)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
vae.compile(optimizer=optimizer)


# Q3 : Train the VAE and collect loss values at specific epochs

epochs_list = [1, 5, 25, 50]  # Epochs for experiments
epochs_to_save = set(epochs_list)
epochs_to_save.add(0)  # Save state after the 1st epoch

for epoch in range(1, 71): #run training for 70 epochs
    history = vae.fit(x_train, x_train, epochs=1, batch_size=batch_size, validation_data=(x_test, x_test), verbose=0)

    if epoch in epochs_to_save or np.all(vae.predict(x_test[:10]) == vae.predict(x_test[10:20])): #second condtion is True when convergence is reached 

        # Plotting the latent vector representation of a few batches of data : encode the test set and mark the different classes 
        x_test_encoded = encoder.predict(x_test, batch_size=batch_size)[2]
        plt.figure(figsize=(6, 6))
        plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)
        plt.colorbar()
        plt.title(f"Epoch {epoch} - Latent Vector Representation")
        plt.show()

        # Plotting some of the reconstructed digits from the latent space (15) and the corresponding original ones 
        reconstructed_images = vae.predict(x_test[:15]) # Generate 15 reconstructed images
        plt.figure(figsize=(10, 4))

        for i in range(15):
            # Original images
            ax = plt.subplot(2, 15, i + 1)
            plt.imshow(x_test[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)

            # Reconstructed images
            ax = plt.subplot(2, 15, i + 16)
            plt.imshow(reconstructed_images[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.suptitle(f"Epoch {epoch} - Original and Reconstructed Digits")
        plt.show()


        # Plotting some of the generated digits (15) 

        random_samples = np.random.normal(size=(15, latent_dim)) # Generate 15 samples from the prior distribution
        generated_images = decoder.predict(random_samples) 
        plt.figure(figsize=(10, 4))
        for i in range(15):
            ax = plt.subplot(3, 5, i + 1)
            plt.imshow(generated_images[i].reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.suptitle(f"Epoch {epoch} - Generated Digits")
        plt.show()


if epoch == 50 or np.all(vae.predict(x_test[:10]) == vae.predict(x_test[10:20])):
        break  # Break if convergence is detected before 50 epochs


# Q4 : Plotting the loss curve (test set), i.e., epoch vs. −LELBO

# Extract loss values
history = vae.fit(x_train, x_train, epochs=60, batch_size=batch_size, validation_data=(x_test, x_test), verbose=0)
train_loss = history.history['loss']
val_loss = history.history['val_loss']

# Plot loss curve
epochs = range(1, len(train_loss) + 1)
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('-LELBO')
plt.title('Loss Curve')
plt.legend()
plt.show()
